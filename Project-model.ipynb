import requests
import random
import pandas as pd
import sqlite3
import networkx as nx
import torch
import torch.nn as nn
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Function to get geospatial data from OpenStreetMap using Overpass API
def get_osm_data(location_query):
    overpass_url = "http://overpass-api.de/api/interpreter"
    overpass_query = f"""
    [out:json];
    area[name="{"chennai"}"]->.a;
    (node["highway"](area.a);
    way["highway"](area.a);
    relation["highway"](area.a););
    out body;
    >;
    out skel qt;
    """
    response = requests.get(overpass_url, params={'data': overpass_query})
    osm_data = response.json()
    return osm_data

# Function to simulate or load traffic data
def load_simulated_traffic_data():
    data = {
        'timestamp': pd.date_range(start='2024-01-01', periods=100, freq='h'),
        'location': ['Chennai'] * 100,
        'traffic_speed': [50 + i % 20 for i in range(100)],  # Renamed to traffic_speed
        'density': [30 + i % 10 for i in range(100)]  # Simulated density data
    }
    df = pd.DataFrame(data)
    return df

# Function to load historical weather data (replace with actual data)
def load_historical_weather_data():
  # Define a list of weather conditions
    weather_conditions = ['clear sky', 'rainy', 'windy', 'thunderstorm', 'cloudy']

    # Map each condition to a numeric value
    weather_map = {
        'clear sky': 0,
        'rainy': 1,
        'windy': 2,
        'thunderstorm': 3,
        'cloudy': 4
    }
    data = {
        'timestamp': pd.date_range(start='2024-01-01', periods=100, freq='h'),
        'temperature': [25 + i % 5 for i in range(100)],  # Simulated temperature data
        'weather_description': [random.choice(weather_conditions) for _ in range(100)],  # Random weather
    }
    df = pd.DataFrame(data)

    # Add a numeric column for the weather conditions
    df['weather_code'] = df['weather_description'].map(weather_map)
    return df

# Load datasets
traffic_df = load_simulated_traffic_data()
weather_df = load_historical_weather_data()

# Merge datasets on timestamp
combined_df = pd.merge(traffic_df, weather_df, on='timestamp')

# Define congestion levels based on density
def define_congestion_level(density):
    if density < 35:
        return "Low"
    elif 35 <= density < 45:
        return "Medium"
    else:
        return "High"

combined_df['congestion_level'] = combined_df['density'].apply(define_congestion_level)


label_encoder = LabelEncoder()
combined_df['weather_encoded'] = label_encoder.fit_transform(combined_df['weather_description'])
combined_df['time_of_day'] = combined_df['timestamp'].dt.hour



# Normalize features using MinMaxScaler
scaler = MinMaxScaler()
combined_df[['traffic_speed', 'density', 'temperature']] = scaler.fit_transform(
    combined_df[['traffic_speed', 'density', 'temperature']])

# Features and target
features = combined_df[['traffic_speed', 'density','temperature', 'time_of_day', 'weather_encoded']]
target = combined_df['congestion_level']

# Convert target to numeric
target = label_encoder.fit_transform(target)
print(features)
print(target)

# Graph-Based Feature Extraction (GNN)
# Creating a simple graph representation
G = nx.Graph()
for i in range(len(combined_df) - 1):
    G.add_edge(i, i+1)
print(G)
# Convert graph to PyTorch Geometric Data object
edge_index = torch.tensor(list(G.edges)).t().contiguous()
x = torch.tensor(combined_df[['traffic_speed', 'density', 'temperature','time_of_day','weather_encoded']].values, dtype=torch.float)
data = Data(x=x, edge_index=edge_index)

print(f"x shape: {data.x.shape}")  # Should print (100, 5)
print(f"Edge index shape: {data.edge_index.shape}")  # Should print (2, number_of_edges)




# Simple GCN model for feature extraction
class GCN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = self.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Initialize and run GCN model
gcn = GCN(input_dim=5, hidden_dim=16, output_dim=8)
gcn_features = gcn(data)

# Autoencoder-Based Dimensionality Reduction
class Autoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),  # Adjust input_dim here
            nn.ReLU(),
            nn.Linear(hidden_dim, 32),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(32, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

# Example of initializing the Autoencoder
autoencoder = Autoencoder(input_dim=8, hidden_dim=16)
encoded_features, _ = autoencoder(gcn_features)

# Combining GCN and Autoencoder features
combined_features = torch.cat([gcn_features, encoded_features], dim=1)

# Train/Test Split for further modeling
X_train, X_test = train_test_split(combined_features.detach().numpy(), test_size=0.2, random_state=42)
y_train, y_test = train_test_split(combined_df['congestion_level'], test_size=0.2, random_state=42)

#  LSTM-CNN model using X_train and y_train

print("Feature extraction complete. Combined features ready for modeling.")
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Encode target variables
y_train_encoded = label_encoder.fit_transform(y_train)  # Convert to numeric
y_test_encoded = label_encoder.transform(y_test)  # Convert to numeric

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.float32).view(-1, 1)

# Create TensorDatasets and DataLoaders
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)


# Define the LSTM-CNN Hybrid Model
class LSTMCNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, lstm_layers, cnn_out_channels, output_dim):
        super(LSTMCNN, self).__init__()

        # LSTM layer
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=lstm_layers, batch_first=True)

        # CNN layer
        self.cnn = nn.Conv1d(in_channels=1, out_channels=cnn_out_channels, kernel_size=3, stride=1, padding=1)

        # Fully connected layers
        self.fc1 = nn.Linear(hidden_dim * cnn_out_channels, 64)
        self.fc2 = nn.Linear(64, output_dim)

        self.relu = nn.ReLU()

    def forward(self, x):
        # Reshape for LSTM layer
        x = x.unsqueeze(1)  # Adding a channel dimension for CNN
        lstm_out, (hn, cn) = self.lstm(x)

        # Apply CNN
        cnn_out = self.cnn(lstm_out)

        # Flatten and apply fully connected layers
        cnn_out = cnn_out.view(cnn_out.size(0), -1)  # Flatten for FC layers
        x = self.relu(self.fc1(cnn_out))
        x = self.fc2(x)

        return x

# Initialize model, loss function, and optimizer
model = LSTMCNN(input_dim=combined_features.shape[1], hidden_dim=32, lstm_layers=2, cnn_out_channels=16, output_dim=1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 50
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * X_batch.size(0)

    train_loss /= len(train_loader.dataset)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}")

# Evaluation
model.eval()
test_loss = 0.0
with torch.no_grad():
    for X_batch, y_batch in test_loader:
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        test_loss += loss.item() * X_batch.size(0)

test_loss /= len(test_loader.dataset)
print(f"Test Loss: {test_loss:.4f}")
print("Model training and evaluation complete.")
